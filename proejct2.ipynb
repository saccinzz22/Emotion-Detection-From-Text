{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /opt/anaconda3/lib/python3.12/site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /opt/anaconda3/lib/python3.12/site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in /opt/anaconda3/lib/python3.12/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in /opt/anaconda3/lib/python3.12/site-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.48.3)\n",
      "Requirement already satisfied: tokenizers in /opt/anaconda3/lib/python3.12/site-packages (0.21.0)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (1.4.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2023.11.17)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "%pip install contractions\n",
    "%pip install transformers tokenizers torch scikit-learn\n",
    "\n",
    "# Import required libraries\n",
    "import re  # Used for removing whitespaces, punctuation, capitalization\n",
    "import contractions  # Used for expanding contractions\n",
    "import nltk  # Used to remove stop words\n",
    "import pandas as pd\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from nltk.corpus import stopwords\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sachin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/sachin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                               text  label\n",
      "0           0      i just feel really helpless and heavy hearted      4\n",
      "1           1  ive enjoyed being able to slouch about relax a...      0\n",
      "2           2  i gave up my internship with the dmrg and am f...      4\n",
      "3           3                         i dont know i feel so lost      0\n",
      "4           4  i am a kindergarten teacher and i am thoroughl...      4\n",
      "\n",
      "Checking for missing values:\n",
      "Unnamed: 0    0\n",
      "text          0\n",
      "label         0\n",
      "dtype: int64\n",
      "\n",
      "Dropping unnecessary index column...\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK stopwords \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "###### STEP 1: LOAD DATASET ######\n",
    "data = pd.read_csv(\"/Users/sachin/Desktop/text.csv\")  \n",
    "\n",
    "print(data.head())\n",
    "\n",
    "print(\"\\nChecking for missing values:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Drop unnecessary columns (e.g., index/serial number)\n",
    "if 'label' in data.columns and 'text' in data.columns:\n",
    "    print(\"\\nDropping unnecessary index column...\")\n",
    "    data = data[['label', 'text']]  # Retain only 'label' and 'text'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                               text\n",
      "0      4      i just feel really helpless and heavy hearted\n",
      "1      0  ive enjoyed being able to slouch about relax a...\n",
      "2      4  i gave up my internship with the dmrg and am f...\n",
      "3      0                         i dont know i feel so lost\n",
      "4      4  i am a kindergarten teacher and i am thoroughl...\n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Preprocessed text examples:\n",
      "0    [2, 24, 187, 82, 230, 1029, 91, 3662, 6298, 3,...\n",
      "1    [2, 24, 146, 2957, 294, 751, 89, 13210, 324, 1...\n",
      "2    [2, 24, 2361, 217, 109, 12825, 148, 88, 19149,...\n",
      "3    [2, 24, 173, 158, 220, 24, 82, 131, 939, 3, 0,...\n",
      "4    [2, 24, 137, 16, 13908, 3305, 91, 24, 137, 567...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "###PREPROCESSING###\n",
    "\n",
    "# Initialize tokenizer \n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "\n",
    "# Train tokenizer on your dataset (if required)\n",
    "tokenizer.train(\n",
    "    files=[\"/Users/sachin/Desktop/text.csv\"],  # Path to your dataset file\n",
    "    vocab_size=30000,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    ")\n",
    "tokenizer.save_model(\"/Users/sachin/Documents\")\n",
    "tokenizer = BertWordPieceTokenizer(\"/Users/sachin/Documents/vocab.txt\")\n",
    "\n",
    "# Define maximum sequence length\n",
    "MAX_SEQ_LEN = 20\n",
    "\n",
    "# Text cleaning function\n",
    "def textCleaner(s):\n",
    "    if not isinstance(s, str) or not s.strip():  # Handle empty or invalid input\n",
    "        return [tokenizer.token_to_id(\"[PAD]\")] * MAX_SEQ_LEN  # Return padded sequence\n",
    "    \n",
    "    # Clean the text\n",
    "    s = re.sub(r'[^A-Za-z0-9\\s]', '', s).lower()  # Remove non-alphanumeric characters and convert to lowercase\n",
    "    s = contractions.fix(s)  # Expand contractions (e.g., \"don't\" -> \"do not\")\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()  # Remove extra whitespaces and strip leading/trailing spaces\n",
    "    s = re.sub(r'\\d+', '', s)  # Remove numbers from text\n",
    "\n",
    "    # Tokenize and convert to token IDs\n",
    "    tokens = tokenizer.encode(s).ids  # Convert tokens to token IDs\n",
    "\n",
    "    # Pad or truncate to MAX_SEQ_LEN\n",
    "    if len(tokens) < MAX_SEQ_LEN:\n",
    "        tokens += [tokenizer.token_to_id(\"[PAD]\")] * (MAX_SEQ_LEN - len(tokens))  # Pad with [PAD]\n",
    "    else:\n",
    "        tokens = tokens[:MAX_SEQ_LEN]  # Truncate to MAX_SEQ_LEN\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "data['text'] = data['text'].apply(textCleaner)\n",
    "\n",
    "\n",
    "print(\"\\nPreprocessed text examples:\")\n",
    "print(data['text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 20])\n",
      "torch.Size([16, 20])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###### STEP 3: DATASET CLASS ######\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.texts[idx], dtype=torch.long)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        # Create attention mask: 1 for non-padding tokens, 0 for padding tokens\n",
    "        attention_mask = (input_ids != tokenizer.token_to_id(\"[PAD]\")).long()\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"label\": label\n",
    "        }\n",
    "\n",
    "\n",
    "###### STEP 4: SPLIT DATA INTO TRAINING AND VALIDATION SETS ######\n",
    "\n",
    "texts = list(data['text'])\n",
    "labels = list(data['label'])\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TextDataset(train_texts, train_labels)\n",
    "val_dataset = TextDataset(val_texts, val_labels)\n",
    "\n",
    "###### STEP 5: CUSTOM COLLATE FUNCTION FOR PADDING ######\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Extract input_ids, attention_masks, and labels from the batch\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    attention_masks = [item[\"attention_mask\"] for item in batch]\n",
    "    labels = [item[\"label\"] for item in batch]\n",
    "\n",
    "    # Pad sequences\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.token_to_id(\"[PAD]\"))\n",
    "    attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)  # Padding value for mask is 0\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_masks,\n",
    "        \"label\": labels\n",
    "    }\n",
    "\n",
    "###### STEP 6: DATALOADERS ######\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "for batch in train_loader:\n",
    "    print(batch[\"input_ids\"].shape)       # Should be [batch_size, seq_len]\n",
    "    print(batch[\"attention_mask\"].shape) # Should be [batch_size, seq_len]\n",
    "    print(batch[\"label\"].shape)          # Should be [batch_size]\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, num_classes, ff_dim):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(embed_dim, num_heads, ff_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input token IDs of shape [batch_size, seq_len].\n",
    "            attention_mask (torch.Tensor): Attention mask of shape [batch_size, seq_len].\n",
    "                                             1 for valid tokens, 0 for padding tokens.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output logits of shape [batch_size, num_classes].\n",
    "        \"\"\"\n",
    "        # Embed the input token IDs\n",
    "        x = self.embedding(x)  # Shape: [batch_size, seq_len, embed_dim]\n",
    "\n",
    "        # Permute dimensions for TransformerEncoderLayer (expects [seq_len, batch_size, embed_dim])\n",
    "        x = x.permute(1, 0, 2)  # Shape: [seq_len, batch_size, embed_dim]\n",
    "\n",
    "        # Apply each Transformer layer\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_key_padding_mask=attention_mask == 0)  # Directly pass binary mask\n",
    "\n",
    "        # Permute back to [batch_size, seq_len, embed_dim]\n",
    "        x = x.permute(1, 0, 2)  # Shape: [batch_size, seq_len, embed_dim]\n",
    "\n",
    "        # Pooling: Take mean over sequence length\n",
    "        x = x.mean(dim=1)  # Shape: [batch_size, embed_dim]\n",
    "\n",
    "        # Classification head\n",
    "        return self.classifier(x)  # Shape: [batch_size, num_classes]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized and moved to device: mps\n"
     ]
    }
   ],
   "source": [
    "# Define constants for the Transformer model\n",
    "VOCAB_SIZE = 30000          # Vocabulary size (number of unique tokens in your tokenizer)\n",
    "EMBEDDING_SIZE = 512        # Size of the embedding vector for each token\n",
    "NUM_CLASSES = 6             # Number of output classes (e.g., emotions: sadness, joy, etc.)\n",
    "NUM_HEADS = 8               # Number of attention heads in each Transformer layer\n",
    "NUM_LAYERS = 6              # Number of Transformer encoder layers\n",
    "FF_DIM = 2048               # Size of the feedforward network inside each Transformer layer\n",
    "\n",
    "# Initialize the model\n",
    "model = TransformerModel(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=EMBEDDING_SIZE,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    ff_dim=FF_DIM,\n",
    ")\n",
    "\n",
    "# Move the model to the appropriate device (e.g., MPS for Mac GPU)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model initialized and moved to device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5):\n",
    "    model.to(device)  # Ensure the model is on the correct device\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)         # Move input tensors to device\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)  # Move attention masks to device\n",
    "            labels = batch[\"label\"].to(device)               # Move labels to device\n",
    "\n",
    "            optimizer.zero_grad()                            # Clear previous gradients\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)  # Forward pass\n",
    "            loss = criterion(outputs, labels)               # Compute loss\n",
    "            loss.backward()                                  # Backpropagation\n",
    "            optimizer.step()                                 # Update model parameters\n",
    "\n",
    "            total_train_loss += loss.item()                  # Accumulate training loss\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_loss_history.append(avg_train_loss)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        total_val_loss = 0\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation during validation\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"label\"].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)  # Forward pass\n",
    "                loss = criterion(outputs, labels)                         # Compute loss\n",
    "\n",
    "                total_val_loss += loss.item()                             # Accumulate validation loss\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_loss_history.append(avg_val_loss)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    return train_loss_history, val_loss_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Training Loss: 1.5787\n",
      "Epoch 1/5, Validation Loss: 1.5792\n",
      "Epoch 2/5, Training Loss: 1.5739\n",
      "Epoch 2/5, Validation Loss: 1.5765\n",
      "Epoch 3/5, Training Loss: 1.5738\n",
      "Epoch 3/5, Validation Loss: 1.5747\n",
      "Epoch 4/5, Training Loss: 1.5735\n",
      "Epoch 4/5, Validation Loss: 1.5761\n",
      "Epoch 5/5, Training Loss: 1.5733\n",
      "Epoch 5/5, Validation Loss: 1.5750\n",
      "Model weights saved successfully!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5  # Number of epochs for training\n",
    "train_loss_history, val_loss_history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    ")\n",
    "\n",
    "# Save the trained model weights\n",
    "torch.save(model.state_dict(), \"transformer_model_weights.pth\")\n",
    "print(\"Model weights saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "def evaluate_model_on_validation(model, val_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluates the trained model on the validation set and computes accuracy and F1 score.\n",
    "\n",
    "    Args:\n",
    "        model: Trained PyTorch model.\n",
    "        val_loader: DataLoader for the validation set.\n",
    "        device: Device (CPU/GPU) where the model is loaded.\n",
    "\n",
    "    Returns:\n",
    "        accuracy: Validation accuracy as a float.\n",
    "        f1_score: Validation F1-score as a float.\n",
    "        Prints a classification report with precision, recall, and F1-score for each class.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_preds = []  # To store all predictions\n",
    "    all_labels = []  # To store all ground truth labels\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)         # Move input IDs to device\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)  # Move attention mask to device\n",
    "            labels = batch[\"label\"].to(device)               # Move labels to device\n",
    "\n",
    "            # Forward pass through the model (fixed)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs, dim=1)  # Get predicted labels (highest logit)\n",
    "\n",
    "            # Collect predictions and true labels\n",
    "            all_preds.extend(preds.cpu().numpy())  # Move predictions to CPU and store\n",
    "            all_labels.extend(labels.cpu().numpy())  # Move labels to CPU and store\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")  # Weighted F1 for multiclass classification\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Validation F1 Score: {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "\n",
    "    return accuracy, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.3379\n",
      "Validation F1 Score: 0.1706\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     24201\n",
      "           1       0.34      1.00      0.51     28164\n",
      "           2       0.00      0.00      0.00      6929\n",
      "           3       0.00      0.00      0.00     11441\n",
      "           4       0.00      0.00      0.00      9594\n",
      "           5       0.00      0.00      0.00      3033\n",
      "\n",
      "    accuracy                           0.34     83362\n",
      "   macro avg       0.06      0.17      0.08     83362\n",
      "weighted avg       0.11      0.34      0.17     83362\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation set\n",
    "accuracy, f1 = evaluate_model_on_validation(model, val_loader, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
